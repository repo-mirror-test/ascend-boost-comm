/**
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include "kernel_operator.h"
#include "../tiling_data.h"
#include "../../utils/common/kernel/kernel_utils.h"

namespace {
constexpr uint32_t MAX_PROCESS_NUM = 12288 * 8;
constexpr uint32_t BLOCK_SIZE = 16;
constexpr uint32_t BLOCK_BYTE = 32;
constexpr uint32_t INT_BYTE = 4;
} // namespace

using AscendC::HardEvent;

class KvCacheNz {
public:
    __aicore__ inline KvCacheNz(uint32_t batch, uint32_t hiddenSize, uint32_t maxSeqLen, uint32_t batchPerCore)
    {
        this->hiddenSize = hiddenSize;
        this->maxSeqLen = maxSeqLen;
        this->batch = batch;
        this->batchPerCore = batchPerCore;
        this->blockIdx = AscendC::GetBlockIdx();
    }
    __aicore__ inline void Init(__gm__ uint8_t *newKV, __gm__ uint8_t *layerId, __gm__ uint8_t *cacheIn,
                                __gm__ uint8_t *tokenOffset, __gm__ uint8_t *seqLen, __gm__ uint8_t *cacheOut,
                                __gm__ uint8_t *tiling)
    {
        newKVGm = (__gm__ half *)newKV;
        cacheOutGm = (__gm__ half *)cacheOut;
        cacheInGm = (__gm__ half *)cacheIn;
        layerIdGm.SetGlobalBuffer((__gm__ uint32_t *)layerId);
        tokenOffsetGm.SetGlobalBuffer((__gm__ uint32_t *)tokenOffset);
        seqLenGm.SetGlobalBuffer((__gm__ uint32_t *)seqLen);

        /* 申请整块buffer，前部分按照block对齐储存所有batch的tokenOffset与seqlen,后部分划分两块存放数据 */
        /* 7 8 一个block存放8个unit32, tokenOffsetSize与seqSize长度相同 */
        uint32_t tokenOffsetSize = CeilDiv(this->batch * INT_BYTE, BLOCK_BYTE) * BLOCK_BYTE;
        pipe.InitBuffer(outQueue, 1, (MAX_PROCESS_NUM * sizeof(half)));
        AscendC::LocalTensor<half> startUbAddr = outQueue.AllocTensor<half>();
        tokenOffsetGmInUb = startUbAddr.ReinterpretCast<uint32_t>();
        seqLenGmInUb = tokenOffsetGmInUb[tokenOffsetSize / (sizeof(uint32_t))];
        AscendC::LocalTensor<half> cachePerloopUb = startUbAddr[2 * tokenOffsetSize / (sizeof(half))];

        DataCopy(tokenOffsetGmInUb, layerIdGm, {1, 1, 0, 0});
        AscendC::SetFlag<HardEvent::MTE2_S>(EVENT_ID0);
        AscendC::WaitFlag<HardEvent::MTE2_S>(EVENT_ID0);
        this->layerId = tokenOffsetGmInUb.GetValue(0);

        DataCopy(seqLenGmInUb, seqLenGm, {1, static_cast<uint16_t>(CeilDiv(this->batch * INT_BYTE, BLOCK_BYTE)), 0, 0});
        AscendC::SetFlag<HardEvent::MTE2_S>(EVENT_ID0);

        /* 搬移当前处理batch的tokenOffset */
        DataCopy(tokenOffsetGmInUb, tokenOffsetGm[blockIdx * batchPerCore],
                 {1, static_cast<uint16_t>(CeilDiv(this->batchPerCore * INT_BYTE, BLOCK_BYTE)), 0, 0});
        AscendC::SetFlag<HardEvent::MTE2_S>(EVENT_ID1);

        AscendC::WaitFlag<HardEvent::MTE2_S>(EVENT_ID0);
        for (uint32_t i = 0; i < batch; ++i) {  // 总的tokens，需要获取到，进行偏移
            ntokens += seqLenGmInUb.GetValue(i);
        }
        ntokens = CeilDiv(ntokens, BLOCK_SIZE) * BLOCK_SIZE;  // 对齐处理
        for (uint32_t i = 0; i < blockIdx * batchPerCore; ++i) {
            prefixOfNtokens += seqLenGmInUb.GetValue(i);
        }

        AscendC::WaitFlag<HardEvent::MTE2_S>(EVENT_ID1);

        cacheBuff = cacheInGm;
        cacheOut = cacheIn;
        cacheBuff = cacheBuff + this->layerId * this->batch * this->maxSeqLen * this->hiddenSize;
        /* 除去前面使用的固定内存，后面内存存放数据 */
        uint32_t maxTokensPerLoop = (MAX_PROCESS_NUM - (2 * tokenOffsetSize) / sizeof(half)) / hiddenSize;
        WriteKvCacheNz(newKVGm, cacheBuff, cachePerloopUb, maxTokensPerLoop);
        outQueue.FreeTensor(startUbAddr);
    }
    __aicore__ inline void WriteKvCacheNz(__gm__ half *oldCache, __gm__ half *newCache,
                                          AscendC::LocalTensor<half> &cachePerloopUb, uint32_t maxTokensPerLoop)
    {
        AscendC::GlobalTensor<half> oldCacheGm;
        AscendC::GlobalTensor<half> newCacheGm;
        oldCacheGm.SetGlobalBuffer((__gm__ half *)oldCache);
        newCacheGm.SetGlobalBuffer((__gm__ half *)newCache);

        AscendC::LocalTensor<uint32_t> batchSeqLen = seqLenGmInUb[blockIdx * batchPerCore];
        uint32_t batchPreCoreReal = batchPerCore;
        if ((blockIdx + 1) * batchPerCore > this->batch) {
            batchPreCoreReal = this->batch - blockIdx * batchPerCore;
        }
        uint32_t tokenNumInLoop = 0;
        for (uint32_t i = 0; i < batchPreCoreReal; i++) {
            tokenNumInLoop += batchSeqLen.GetValue(i);
        }

        uint32_t maxTokensPerBuffer = maxTokensPerLoop / 2; // 2 写死划分double buffer
        AscendC::LocalTensor<half> cachePerloopUbPart2 = cachePerloopUb[maxTokensPerBuffer * hiddenSize];

        uint32_t loopTimes = CeilDiv(tokenNumInLoop, maxTokensPerBuffer);
        uint32_t tailTokens = tokenNumInLoop % maxTokensPerBuffer;
        uint32_t batchIndex = 0;
        uint32_t tokenNow = 0;

        AscendC::SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
        AscendC::SetFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
        for (uint32_t loop = 0; loop < loopTimes; loop++) {
            uint32_t seqLenSum = 0;
            AscendC::LocalTensor<half> cacheLoopUb = (loop % 2) ? cachePerloopUbPart2 : cachePerloopUb;
            AscendC::LocalTensor<half> cacheUb = cacheLoopUb;
            if (loop == (loopTimes - 1) && (tailTokens != 0)) { // 最后一次且尾块不满
                tokenNow = tailTokens;
                AscendC::WaitFlag<HardEvent::MTE3_MTE2>(loop % 2);
                DataCopy(cacheLoopUb, oldCacheGm[loop * maxTokensPerBuffer * BLOCK_SIZE + prefixOfNtokens * BLOCK_SIZE],
                         {static_cast<uint16_t>(hiddenSize / BLOCK_SIZE), static_cast<uint16_t>(tailTokens),
                          static_cast<uint16_t>(ntokens - tailTokens), 0});
            } else {
                tokenNow = maxTokensPerBuffer;
                AscendC::WaitFlag<HardEvent::MTE3_MTE2>(loop % 2);
                DataCopy(cacheLoopUb, oldCacheGm[loop * maxTokensPerBuffer * BLOCK_SIZE + prefixOfNtokens * BLOCK_SIZE],
                         {static_cast<uint16_t>(hiddenSize / BLOCK_SIZE), static_cast<uint16_t>(maxTokensPerBuffer),
                          static_cast<uint16_t>(ntokens - maxTokensPerBuffer), 0});
            }
            AscendC::SetFlag<HardEvent::MTE2_MTE3>(loop % 2);
            AscendC::WaitFlag<HardEvent::MTE2_MTE3>(loop % 2);

            /* 对于所有的搬入，只对应batchPreCoreReal个batch的搬出 */
            for (; batchIndex < batchPreCoreReal; batchIndex++) {
                uint32_t seqLenNow = batchSeqLen.GetValue(batchIndex);
                uint32_t tokenOffset = tokenOffsetGmInUb.GetValue(batchIndex);
                tokenOffset -= seqLenNow;
                seqLenSum += seqLenNow;
                uint64_t kvCacheNzBatchOffset =
                    (blockIdx * batchPerCore + batchIndex) * maxSeqLen * hiddenSize + tokenOffset * BLOCK_SIZE;
                if (seqLenSum > maxTokensPerBuffer) {
                    uint32_t tailSeqLen = seqLenSum - maxTokensPerBuffer;
                    uint64_t seqMov = maxTokensPerBuffer - (seqLenSum - seqLenNow);
                    if (seqMov == 0) { // ub内搬完后不做搬移
                        break;
                    }
                    DataCopy(newCacheGm[kvCacheNzBatchOffset], cacheUb,
                             {static_cast<uint16_t>(hiddenSize / BLOCK_SIZE), static_cast<uint16_t>(seqMov),
                              static_cast<uint16_t>(tokenNow - seqMov), static_cast<uint16_t>(maxSeqLen - seqMov)});
                    batchSeqLen.SetValue(batchIndex, tailSeqLen);
                    break;
                } else {
                    DataCopy(newCacheGm[kvCacheNzBatchOffset], cacheUb,
                             {static_cast<uint16_t>(hiddenSize / BLOCK_SIZE), static_cast<uint16_t>(seqLenNow),
                              static_cast<uint16_t>(tokenNow - seqLenNow),
                              static_cast<uint16_t>(maxSeqLen - seqLenNow)});
                    cacheUb = cacheUb[seqLenNow * BLOCK_SIZE];
                }
            }
            AscendC::SetFlag<HardEvent::MTE3_MTE2>(loop % 2);
        }
        AscendC::WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID0);
        AscendC::WaitFlag<HardEvent::MTE3_MTE2>(EVENT_ID1);
    }

private:
    /* data */
    __gm__ half *newKVGm{nullptr};
    __gm__ half *cacheInGm{nullptr};
    __gm__ half *cacheOutGm{nullptr};
    __gm__ half *cacheBuff{nullptr};
    AscendC::GlobalTensor<uint32_t> layerIdGm;
    AscendC::GlobalTensor<uint32_t> tokenOffsetGm;
    AscendC::GlobalTensor<uint32_t> seqLenGm;
    AscendC::LocalTensor<uint32_t> tokenOffsetGmInUb;
    AscendC::LocalTensor<uint32_t> seqLenGmInUb;
    uint32_t batch{0};
    uint32_t hiddenSize{0};
    uint32_t maxSeqLen{0};
    uint32_t layerId{0};
    uint32_t prefixOfNtokens{0};
    uint32_t ntokens{0};
    AscendC::TPipe pipe;
    AscendC::TQue<AscendC::QuePosition::VECIN, 1> outQueue;
    uint32_t batchPerCore{0}; // 每核处理batch数
    uint64_t blockIdx{0};
};

inline __aicore__ void InitTilingData(const __gm__ uint8_t *pTilingdata, SimpleOps::KVCacheTilingData *tilingData)
{
    AscendC::TPipe pipe;
    __ubuf__ uint8_t *tilingDataInUb = nullptr;
    CopyGmTilingToUb(tilingDataInUb, pTilingdata, sizeof(SimpleOps::KVCacheTilingData), &pipe);
    AscendC::PipeBarrier<PIPE_ALL>();
    tilingData->batch = (*(__ubuf__ uint32_t *)(tilingDataInUb + 0));
    tilingData->maxSeqLen = (*(__ubuf__ uint32_t *)(tilingDataInUb + 4));
    tilingData->hiddenSize = (*(__ubuf__ uint32_t *)(tilingDataInUb + 8));
    tilingData->batchPerCore = (*(__ubuf__ uint32_t *)(tilingDataInUb + 12));
}

extern "C" __global__ __aicore__ void kvcache_nz(GM_ADDR newKV, GM_ADDR layerId, GM_ADDR cacheIn, GM_ADDR tokenOffset,
                                                 GM_ADDR seqLen, GM_ADDR cacheOut, GM_ADDR tiling)
{
    SimpleOps::KVCacheTilingData tilingData;
    InitTilingData(tiling, &(tilingData));
    KvCacheNz op(tilingData.batch, tilingData.hiddenSize, tilingData.maxSeqLen, tilingData.batchPerCore);
    op.Init(newKV, layerId, cacheIn, tokenOffset, seqLen, cacheOut, tiling);
}
